{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf23f700",
   "metadata": {},
   "source": [
    "#### Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe8d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda61c26",
   "metadata": {},
   "source": [
    "Lets implement a simple self-attention for a tiny input.\n",
    "- Define Queries $(Q)$, Keys $(K)$, and Values $(V)$ by projecting input vectors.\n",
    "- Compute attention scores = $Q * K^T$\n",
    "- Scale scores and apply softmax to get attention weights\n",
    "- Compute weighted sum of V using the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9b8a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:(2, 5, 8), K:(2, 5, 8), V:(2, 5, 8)\n",
      "\n",
      "[[[2.81348661 2.04906146 0.98956203 2.24095702 2.00129809 1.50077431\n",
      "   2.16213921 1.67483965]\n",
      "  [3.079837   2.04442056 1.41953078 2.24569558 1.86390987 1.03422676\n",
      "   2.45132443 1.64541432]\n",
      "  [2.09183202 1.72092826 0.98099999 1.62972733 1.48519581 1.05538966\n",
      "   1.96457732 1.37443833]\n",
      "  [2.34229407 1.81915244 1.16653628 1.82111387 1.58675011 1.13408538\n",
      "   2.37428008 1.47010381]\n",
      "  [2.73161844 2.09559238 1.16096713 2.2518049  1.73014859 1.67088853\n",
      "   2.53262628 1.73339155]]\n",
      "\n",
      " [[2.43541131 1.87891425 1.16746657 1.92474185 1.52069436 0.97693226\n",
      "   2.07054    1.45346065]\n",
      "  [3.9245769  3.55037243 2.01019131 2.80272973 2.43640851 1.94970088\n",
      "   3.663497   2.8054415 ]\n",
      "  [1.3404496  1.47947343 0.99550988 1.29542652 1.06938915 0.75063337\n",
      "   1.85184806 1.0470242 ]\n",
      "  [2.5002975  2.17031702 1.24631062 1.56444293 1.25689034 1.2935675\n",
      "   2.60090763 1.84712708]\n",
      "  [2.59881267 1.82663492 1.12032449 2.08153862 1.60233091 1.29118413\n",
      "   2.21074913 1.51364117]]]\n"
     ]
    }
   ],
   "source": [
    "x= np.random.rand(2,5,8)\n",
    "d_model = x.shape[2]\n",
    "\n",
    "wq = np.random.rand(d_model, d_model)\n",
    "wk = np.random.rand(d_model, d_model)\n",
    "wv = np.random.rand(d_model, d_model)\n",
    "\n",
    "Q = x @ wq\n",
    "K = x @ wk\n",
    "V = x @ wv\n",
    "\n",
    "print(f\"Q:{Q.shape}, K:{K.shape}, V:{V.shape}\\n\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9383b65",
   "metadata": {},
   "source": [
    "Lets compute the attention scores, scale them and apply softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af19670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention score shape: (2, 5, 5)\n",
      "[[[25.60465863 25.10269067 21.50456811 25.11184948 31.32003528]\n",
      "  [26.35741741 26.42247042 22.35663787 26.06745047 32.09337878]\n",
      "  [20.56056319 20.62310971 17.40520468 20.40647158 25.22323214]\n",
      "  [22.92875359 23.19098238 19.48989398 22.87787737 28.12536617]\n",
      "  [26.49358999 26.30407363 22.38241025 26.21700367 32.52453393]]\n",
      "\n",
      " [[20.22747003 36.84227124 17.95817877 24.6079159  22.88059561]\n",
      "  [35.12814843 63.68983939 31.15911807 42.91357857 40.09510641]\n",
      "  [15.06468876 27.22022164 13.20414129 18.3583466  17.22272494]\n",
      "  [21.9314704  40.03070232 19.63388768 26.87695165 25.3191665 ]\n",
      "  [21.23287801 38.8330103  18.98995259 25.92628467 24.12113112]]]\n"
     ]
    }
   ],
   "source": [
    "attention_score = np.matmul(Q, K.transpose(0, 2, 1))\n",
    "print(f\"Attention score shape: {attention_score.shape}\\n{attention_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3de90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.04815372272715"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(attention_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9d588",
   "metadata": {},
   "source": [
    "When the embedding dimension d_model is large, the dot products of Q and K vectors can grow large in magnitude. Hence, Dividing by $√d_model$ normalizes the dot product to keep it in a range that produces a smoother, more stable softmax distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9beaceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled (d) Attention score shape:\n",
      "[[[3.20058233 3.13783633 2.68807101 3.13898118 3.91500441]\n",
      "  [3.29467718 3.3028088  2.79457973 3.25843131 4.01167235]\n",
      "  [2.5700704  2.57788871 2.17565058 2.55080895 3.15290402]\n",
      "  [2.8660942  2.8988728  2.43623675 2.85973467 3.51567077]\n",
      "  [3.31169875 3.2880092  2.79780128 3.27712546 4.06556674]]\n",
      "\n",
      " [[2.52843375 4.60528391 2.24477235 3.07598949 2.86007445]\n",
      "  [4.39101855 7.96122992 3.89488976 5.36419732 5.0118883 ]\n",
      "  [1.8830861  3.4025277  1.65051766 2.29479332 2.15284062]\n",
      "  [2.7414338  5.00383779 2.45423596 3.35961896 3.16489581]\n",
      "  [2.65410975 4.85412629 2.37374407 3.24078558 3.01514139]]]\n",
      "\n",
      "Scaled (sqrt(d)) Attention score shape:\n",
      "[[[ 9.05261387  8.8751414   7.60301297  8.87837953 11.07330467]\n",
      "  [ 9.31875429  9.34175401  7.90426512  9.2162355  11.34672288]\n",
      "  [ 7.26925683  7.29137036  6.15366913  7.21477722  8.91775925]\n",
      "  [ 8.10653858  8.19925045  6.8907181   8.08855111  9.94381857]\n",
      "  [ 9.36689857  9.29989442  7.91337703  9.26911054 11.49915925]]\n",
      "\n",
      " [[ 7.15149061 13.02570992  6.34917499  8.7002121   8.08951216]\n",
      "  [12.41967598 22.51775866 11.01641184 15.1722412  14.17576082]\n",
      "  [ 5.32617179  9.62380165  4.66836892  6.49065569  6.0891528 ]\n",
      "  [ 7.75394572 14.15299053  6.94162756  9.50243738  8.95167716]\n",
      "  [ 7.50695601 13.72954246  6.71396212  9.16632585  8.52810769]]]\n"
     ]
    }
   ],
   "source": [
    "scaled_with_d = attention_score / d_model\n",
    "scaled_with_sqrtd = attention_score / np.sqrt(d_model)\n",
    "print(f\"Scaled (d) Attention score shape:\\n{scaled_with_d}\\n\")\n",
    "print(f\"Scaled (sqrt(d)) Attention score shape:\\n{scaled_with_sqrtd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ff31e",
   "metadata": {},
   "source": [
    "**Why use $√d$ and not $d$ ?**\n",
    "- Dividing by $√d$ normalizes the variance of the dot product to be roughly constant with respect to $d$.\n",
    "- Dividing by d would shrink the scores too much, making the softmax output too uniform (values too close together), leading to less confident attention distributions and potentially harming learning.\n",
    "- Dividing by $√d$ achieves a balance: it prevents scores from getting too large (which would make softmax saturate and gradients vanish) without shrinking them too much.\n",
    "\n",
    "| Scale factor | Effect on scores                                           | Resulting softmax distribution |\n",
    "| ------------ | ---------------------------------------------------------- | ------------------------------ |\n",
    "| No scaling   | Large scores → saturate softmax → vanishing gradients      |                                |\n",
    "| Divide by d  | Scores too small → softmax nearly uniform → weak attention |                                |\n",
    "| Divide by √d | Scores normalized → stable gradients → effective attention |                                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de949a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.881019215340894"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(scaled_with_sqrtd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2296613c",
   "metadata": {},
   "source": [
    "The key is the variance does not grow proportional to $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e82774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x-np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15705af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention: (2, 5, 5)\n",
      "[[[9.56417750e-02 8.00888970e-02 2.24436741e-02 8.03486552e-02\n",
      "   7.21476999e-01]\n",
      "  [9.28719660e-02 9.50327481e-02 2.25725071e-02 8.38226318e-02\n",
      "   7.05700147e-01]\n",
      "  [1.17699012e-01 1.20330744e-01 3.85726284e-02 1.11458353e-01\n",
      "   6.11939263e-01]\n",
      "  [1.03570913e-01 1.13632368e-01 3.07053237e-02 1.01724590e-01\n",
      "   6.50366805e-01]\n",
      "  [8.68833149e-02 8.12525219e-02 2.03086006e-02 7.87893612e-02\n",
      "   7.32766201e-01]]\n",
      "\n",
      " [[2.74381933e-03 9.76105006e-01 1.23002592e-03 1.29109057e-02\n",
      "   7.01024332e-03]\n",
      "  [4.11199442e-05 9.99065824e-01 1.01070088e-05 6.44876282e-04\n",
      "   2.38073179e-04]\n",
      "  [1.24390117e-02 9.14582341e-01 6.44326097e-03 3.98579362e-02\n",
      "   2.66774502e-02]\n",
      "  [1.63459379e-03 9.82832922e-01 7.25478506e-04 9.39226042e-03\n",
      "   5.41474489e-03]\n",
      "  [1.94745757e-03 9.81528606e-01 8.81201312e-04 1.02358324e-02\n",
      "   5.40690258e-03]]]\n"
     ]
    }
   ],
   "source": [
    "attention_weight = softmax(scaled_with_sqrtd)\n",
    "print(f\"attention: {attention_weight.shape}\\n{attention_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50145949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: (2, 5, 8)\n",
      "[[[2.31118748 2.43526663 2.60021598 2.29518082 2.23936964 2.13753245\n",
      "   1.94199825 1.97020266]\n",
      "  [2.30583572 2.43189857 2.58808358 2.29171588 2.22366791 2.12819057\n",
      "   1.93458963 1.96467516]\n",
      "  [2.26086411 2.39727201 2.53287438 2.26875285 2.15696284 2.07288757\n",
      "   1.90142164 1.93494217]\n",
      "  [2.28007454 2.41227043 2.55392498 2.27735632 2.18197051 2.09611181\n",
      "   1.91318812 1.94656405]\n",
      "  [2.31723583 2.43992623 2.60447032 2.29587276 2.24382477 2.14487682\n",
      "   1.94302629 1.97285133]]\n",
      "\n",
      " [[3.84598117 3.57303707 3.57988203 2.85057197 3.01064801 2.84959987\n",
      "   3.02204099 3.24750381]\n",
      "  [3.88028566 3.60169024 3.61961934 2.87354785 3.03492777 2.87576409\n",
      "   3.05992551 3.28398129]\n",
      "  [3.75145444 3.49405481 3.47338966 2.78740291 2.94104723 2.77874597\n",
      "   2.92033759 3.14940679]\n",
      "  [3.85601952 3.58152674 3.59187022 2.85743392 3.01790511 2.85756595\n",
      "   3.03327132 3.25832444]\n",
      "  [3.85416633 3.579883   3.58931042 2.85604928 3.0165523  2.85585402\n",
      "   3.0309997  3.25614541]]]\n"
     ]
    }
   ],
   "source": [
    "output = np.matmul(attention_weight, V)\n",
    "print(f\"Output: {output.shape}\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4978a",
   "metadata": {},
   "source": [
    "Lets create a self attention on mechanism using pytorch and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c28069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7485a9012330>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F \n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e8e3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.wq = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.wk = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.wv = nn.Linear(d_in, d_out, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.wq(x)\n",
    "        K = self.wk(x)\n",
    "        V = self.wv(x)\n",
    "\n",
    "        attention_score = Q @ K.transpose(2,1) / np.sqrt(self.d_in)\n",
    "        attention_weight = F.softmax(attention_score, dim=-1)\n",
    "\n",
    "        output = attention_weight @ V\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "debe9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(2,5,8)\n",
    "d_in, d_out = x.shape[-1], x.shape[-1]\n",
    "self_attn = SelfAttention(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef43bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 8])\n",
      "tensor([[[ 0.0207, -0.0072,  0.0424,  0.6898,  0.1978,  0.4199,  0.0109,\n",
      "           0.6590],\n",
      "         [ 0.2304, -0.0192,  0.0728,  0.1547,  0.0503, -0.0697, -0.1201,\n",
      "           0.1993],\n",
      "         [ 0.3303,  0.0403, -0.0099, -0.0942,  0.0513, -0.3193, -0.2431,\n",
      "           0.0046],\n",
      "         [ 0.3393,  0.0349, -0.0301, -0.1476,  0.0267, -0.3847, -0.2255,\n",
      "          -0.0411],\n",
      "         [ 0.1075, -0.2178,  0.2254,  0.2262, -0.1503,  0.0407,  0.2039,\n",
      "           0.2271]],\n",
      "\n",
      "        [[ 0.2868,  0.4197,  0.5635,  0.1785, -0.1162, -0.1099, -0.7239,\n",
      "          -0.5140],\n",
      "         [ 0.2571,  0.3941,  0.5608,  0.2562, -0.0864, -0.0646, -0.6615,\n",
      "          -0.4587],\n",
      "         [ 0.1722,  0.4810,  0.4679, -0.0181, -0.2029, -0.0472, -0.9151,\n",
      "          -0.6055],\n",
      "         [ 0.2445,  0.4674,  0.5278,  0.0388, -0.1765, -0.0824, -0.8920,\n",
      "          -0.5864],\n",
      "         [ 0.2123,  0.4595,  0.5051,  0.1072, -0.1573, -0.0396, -0.8268,\n",
      "          -0.5240]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = self_attn(X)\n",
    "print(f\"Output shape: {output.shape}\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea02174",
   "metadata": {},
   "source": [
    "![image.png](media/attn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc87cd7",
   "metadata": {},
   "source": [
    "#### Hiding the future words with causal attention \n",
    "In causal attention, the attention weight above diagonlas is masked. This means the LLM can not utilize the future tokens to create context vector. \n",
    "\n",
    "![image.png](media/cmask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7943375",
   "metadata": {},
   "source": [
    "![image](media/cmaskprocess.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7bce61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal attention score shape: torch.Size([2, 5, 5])\n",
      " tensor([[[ 2.9553,  0.9264,  0.0417,  0.6856, -2.0050],\n",
      "         [-0.4110,  0.0301,  1.2666, -0.1143, -0.8055],\n",
      "         [-0.9198,  1.1590,  3.5975,  1.1701, -2.0234],\n",
      "         [-1.6577,  0.7734,  3.6305,  1.1457, -1.4466],\n",
      "         [-1.0227, -1.0987, -2.8207, -1.0246,  2.7426]],\n",
      "\n",
      "        [[-0.2186, -0.7892, -0.1712,  0.4683, -1.5515],\n",
      "         [-0.3396, -0.2679, -0.3899,  0.5105, -0.3881],\n",
      "         [ 0.3817,  0.1084,  0.5549, -1.2537, -1.8937],\n",
      "         [ 0.5263, -0.9256,  0.4854, -0.1689, -2.1769],\n",
      "         [ 0.7852,  0.1437,  0.1935,  0.1885, -1.2148]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# lets use the previous example. \n",
    "\n",
    "queries = self_attn.wq(X)\n",
    "keys = self_attn.wk(X)\n",
    "\n",
    "causal_attention_score = queries @ keys.transpose(1,2) \n",
    "print(f\"causal attention score shape: {causal_attention_score.shape}\\n {causal_attention_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8abe825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape: torch.Size([5, 5])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# lets create a mask\n",
    "mask = torch.tril(torch.ones(causal_attention_score.shape[1], causal_attention_score.shape[2]))\n",
    "print(f\"mask shape: {mask.shape}\\n{mask}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c2e58",
   "metadata": {},
   "source": [
    "we created the mask of size ze (seq_len, seq_len) and this will be broadcasted to batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "030316f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked attention score shape: torch.Size([2, 5, 5])\n",
      "tensor([[[ 2.9553,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [-0.4110,  0.0301,  0.0000, -0.0000, -0.0000],\n",
      "         [-0.9198,  1.1590,  3.5975,  0.0000, -0.0000],\n",
      "         [-1.6577,  0.7734,  3.6305,  1.1457, -0.0000],\n",
      "         [-1.0227, -1.0987, -2.8207, -1.0246,  2.7426]],\n",
      "\n",
      "        [[-0.2186, -0.0000, -0.0000,  0.0000, -0.0000],\n",
      "         [-0.3396, -0.2679, -0.0000,  0.0000, -0.0000],\n",
      "         [ 0.3817,  0.1084,  0.5549, -0.0000, -0.0000],\n",
      "         [ 0.5263, -0.9256,  0.4854, -0.1689, -0.0000],\n",
      "         [ 0.7852,  0.1437,  0.1935,  0.1885, -1.2148]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attention_score = causal_attention_score * mask\n",
    "print(f\"masked attention score shape: {masked_attention_score.shape}\\n{masked_attention_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7096c297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  1.0000,   0.0000,   0.0000,   0.0000,  -0.0000],\n",
      "         [  1.0790,  -0.0790,  -0.0000,   0.0000,   0.0000],\n",
      "         [ -0.2397,   0.3021,   0.9377,   0.0000,  -0.0000],\n",
      "         [ -0.4259,   0.1987,   0.9328,   0.2944,  -0.0000],\n",
      "         [  0.3172,   0.3408,   0.8749,   0.3178,  -0.8507]],\n",
      "\n",
      "        [[  1.0000,   0.0000,   0.0000,  -0.0000,   0.0000],\n",
      "         [  0.5590,   0.4410,   0.0000,  -0.0000,   0.0000],\n",
      "         [  0.3653,   0.1037,   0.5310,  -0.0000,  -0.0000],\n",
      "         [ -6.3572,  11.1813,  -5.8639,   2.0398,   0.0000],\n",
      "         [  8.1838,   1.4973,   2.0165,   1.9646, -12.6622]]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# lets normalize the masked attention score\n",
    "row_sum = masked_attention_score.sum(axis=-1, keepdim=True)\n",
    "norm_attn_score = masked_attention_score / row_sum\n",
    "print(norm_attn_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e938c38",
   "metadata": {},
   "source": [
    "Masking after softmax would be incincorrect as it would change the distribution. Softmax ensures that the probability sum to 1. Effciently, we can set -ve infinity to for masked positions and then apply softmax which will make them 0. \n",
    "![iamge](media/cmsoftmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c5e089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[[ 2.9553,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.4110,  0.0301,    -inf,    -inf,    -inf],\n",
      "         [-0.9198,  1.1590,  3.5975,    -inf,    -inf],\n",
      "         [-1.6577,  0.7734,  3.6305,  1.1457,    -inf],\n",
      "         [-1.0227, -1.0987, -2.8207, -1.0246,  2.7426]],\n",
      "\n",
      "        [[-0.2186,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.3396, -0.2679,    -inf,    -inf,    -inf],\n",
      "         [ 0.3817,  0.1084,  0.5549,    -inf,    -inf],\n",
      "         [ 0.5263, -0.9256,  0.4854, -0.1689,    -inf],\n",
      "         [ 0.7852,  0.1437,  0.1935,  0.1885, -1.2148]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(causal_attention_score.shape[1], causal_attention_score.shape[1]), diagonal=1)\n",
    "print(mask)\n",
    "mask_causal_attention_score  = causal_attention_score.masked_fill(mask.bool(), -torch.inf)\n",
    "print(mask_causal_attention_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cea8b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:torch.Size([2, 5, 5])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4611, 0.5389, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1246, 0.2599, 0.6155, 0.0000, 0.0000],\n",
      "         [0.0797, 0.1883, 0.5171, 0.2148, 0.0000],\n",
      "         [0.1372, 0.1336, 0.0727, 0.1371, 0.5194]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4937, 0.5063, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3366, 0.3056, 0.3578, 0.0000, 0.0000],\n",
      "         [0.2971, 0.1778, 0.2928, 0.2323, 0.0000],\n",
      "         [0.2557, 0.2038, 0.2074, 0.2070, 0.1261]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "causal_attention_weight = F.softmax(mask_causal_attention_score / np.sqrt(self_attn.d_in), dim=-1)\n",
    "print(f\"Shape:{causal_attention_weight.shape}\\n{causal_attention_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13a32a",
   "metadata": {},
   "source": [
    "In addition, we can add droput to prevent overfit. It can be applied to anywhere. This can be applied anywhere, but in transformer, we mostly apply after multihead attention. \n",
    "![image](media/dropout.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa709971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.9222, 1.0778, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.2310, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.0343, 0.0000, 0.0000],\n",
      "         [0.2744, 0.2671, 0.1453, 0.2742, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.9873, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5941, 0.3556, 0.5856, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.4148, 0.0000, 0.0000]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# example \n",
    "dropout = nn.Dropout(0.5)\n",
    "print(dropout(causal_attention_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4417b",
   "metadata": {},
   "source": [
    "Lets create simple attention class with all these above components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c349883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAV2(nn.Module):\n",
    "    def __init__(self, d_model:int, bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.wq(x)\n",
    "        K = self.wk(x)\n",
    "        V =self.wv(x)\n",
    "\n",
    "        attention_score = Q @ K.transpose(1,2)\n",
    "        batch_size, seq_len, _ = attention_score.size()\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        # this happens automatically via broadcasting, Yet lets be explicit \n",
    "        mask = mask.unsqueeze(0).expand(batch_size, -1, -1) # (b, seq, seq)\n",
    "        masked_attention_score = attention_score.masked_fill(mask.bool(), -torch.inf)\n",
    "\n",
    "        attention_weight = F.softmax(masked_attention_score / self.d_model**0.5, dim=-1)\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "        output = attention_weight @ V\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ad49179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:torch.Size([2, 3, 4])\n",
      "tensor([[[ 0.3118, -0.3660, -0.0574, -0.7948],\n",
      "         [ 0.1792, -0.2334, -0.0644, -0.6985],\n",
      "         [ 0.1169, -0.1521, -0.0418, -0.4542]],\n",
      "\n",
      "        [[-0.4388, -0.7512, -0.0193, -1.1770],\n",
      "         [ 0.0764, -0.1267, -0.0145, -0.2312],\n",
      "         [ 0.0494, -0.0818, -0.0094, -0.1493]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(2,3,4)\n",
    "sv2 = SAV2(d_model=4)\n",
    "output = sv2(input)\n",
    "print(f\"Shape:{output.size()}\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c59ba2",
   "metadata": {},
   "source": [
    "This summarizes the whole process above. \n",
    "![image](media/sha.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1891e",
   "metadata": {},
   "source": [
    "Next, we will develop multi-head attention mechanism. The picture are take from [rasbt](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be2b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
