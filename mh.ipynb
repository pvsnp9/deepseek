{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52fcdb8",
   "metadata": {},
   "source": [
    "#### Why Self-attention is insufficient?\n",
    "\n",
    "The main limitation is that a single attention \"head\" can only capture one type of relationship or feature pattern at a time. Real-world data, like language, has complex interactions, including:\n",
    "\n",
    "- Syntax (e.g., subject-verb agreement)\n",
    "- Semantic relationships (e.g., co-reference)\n",
    "- Long-range dependencies (e.g., linking beginning and end of a paragraph)\n",
    "- Different positional patterns\n",
    "\n",
    "Trying to encode all these diverse patterns with one single attention is restrictive and inefficient.\n",
    "\n",
    "`The cat sat on the mat becuase it was tired`\n",
    "\n",
    "1. One attention head might want to focus on resolving pronouns (\"it\" → \"cat\").\n",
    "2. Another head might want to focus on positional relations (which word is next to which).\n",
    "3. Another might focus on important keywords (\"tired\", \"mat\").\n",
    "\n",
    "With only one head, the model tries to mix all these focuses into a single attention pattern, which can dilute the quality and richness of learned features.\n",
    "\n",
    "**How Multi-Head attention solves this?**\n",
    "Instead of having one set of Q, K, V projections, multi-head attention splits the model’s capacity into several heads, each learning different projections:\n",
    "\n",
    "- Each head attends to different parts or aspects of the input independently.\n",
    "- The outputs of all heads are concatenated and linearly transformed, allowing the model to jointly attend to information from different representation subspaces.\n",
    "\n",
    "This way, each attention head can focus on a specific aspect (e.g., syntax vs semantics), reducing the conflict and improving overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda8b17",
   "metadata": {},
   "source": [
    "![MH](media/mh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c5e805c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7be7dc110f70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.functional import F\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ccaa3d",
   "metadata": {},
   "source": [
    "![MH](media/mhqkv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "351b0e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:torch.Size([2, 4, 9]), K: torch.Size([2, 4, 9]), V:torch.Size([2, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(2, 4, 9)\n",
    "b, seq, d_model = input.size()\n",
    "n_heads = 3\n",
    "\n",
    "wq = nn.Linear(d_model, d_model)\n",
    "wk = nn.Linear(d_model, d_model)\n",
    "wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "dropout = nn.Dropout(0.4)\n",
    "\n",
    "# input\n",
    "Q = wq(input)\n",
    "K = wk(input)\n",
    "V = wv(input)   \n",
    "\n",
    "print(f\"Q:{Q.size()}, K: {K.size()}, V:{V.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83e24842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:torch.Size([2, 4, 3, 3]), K: torch.Size([2, 4, 3, 3]), V:torch.Size([2, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# lets make sure the num head is correct\n",
    "assert Q.size(0)//n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "head_dim = d_model // n_heads\n",
    "\n",
    "# lets split heads \n",
    "Q = Q.view(b, seq, n_heads, head_dim)\n",
    "K = K.view(b, seq, n_heads, head_dim)\n",
    "V = V.view(b, seq, n_heads, head_dim)\n",
    "\n",
    "print(f\"Q:{Q.size()}, K: {K.size()}, V:{V.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d36283",
   "metadata": {},
   "source": [
    "**lets transpose the heads to get the right shape for matrix multiplication so that each head can attend to all tokens in the sequence.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e369868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:torch.Size([2, 3, 4, 3]), K: torch.Size([2, 3, 4, 3]), V:torch.Size([2, 3, 4, 3])\n",
      "Q data:\n",
      "tensor([[[[-0.1936, -0.1699,  0.1577],\n",
      "          [-1.3494,  0.1358, -0.4783],\n",
      "          [-0.0148, -0.5717,  0.0037],\n",
      "          [ 0.2234, -0.3064, -0.5063]],\n",
      "\n",
      "         [[ 0.1638, -0.0595,  0.2435],\n",
      "          [ 0.5134,  0.2944,  0.9202],\n",
      "          [-0.1530,  0.1878, -0.7437],\n",
      "          [-0.3977, -0.7100, -0.6491]],\n",
      "\n",
      "         [[-0.0672, -1.8729,  1.1167],\n",
      "          [-0.0991, -0.0460,  1.0454],\n",
      "          [ 0.4609,  0.6065, -0.0997],\n",
      "          [-0.0332, -1.4115,  0.9683]]],\n",
      "\n",
      "\n",
      "        [[[-0.3469,  0.2196, -0.0338],\n",
      "          [-0.4134, -0.3903, -0.3228],\n",
      "          [ 0.0970, -0.2078,  0.5537],\n",
      "          [ 0.2554, -1.3217, -0.7302]],\n",
      "\n",
      "         [[ 0.8727, -0.3630,  1.0433],\n",
      "          [-0.0310, -0.4599,  0.5259],\n",
      "          [ 0.0860,  0.0208, -0.6648],\n",
      "          [-0.4612, -1.1273, -0.9428]],\n",
      "\n",
      "         [[-0.2181,  0.0292,  0.0248],\n",
      "          [ 0.9741, -0.6951,  0.6917],\n",
      "          [ 0.2260, -0.2608, -0.5675],\n",
      "          [ 0.3471, -1.2189,  0.4986]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = Q.transpose(1,2) # (b, n_heads, seq, head_dim)\n",
    "K = K.transpose(1,2) # (b, n_heads, seq, head_dim)\n",
    "V = V.transpose(1,2) # (b, n_heads, seq, head_dim)\n",
    "\n",
    "print(f\"Q:{Q.size()}, K: {K.size()}, V:{V.size()}\\nQ data:\\n{Q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aabb2ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:torch.Size([2, 3, 4, 4])\n",
      "tensor([[[[False,  True,  True,  True],\n",
      "          [False, False,  True,  True],\n",
      "          [False, False, False,  True],\n",
      "          [False, False, False, False]],\n",
      "\n",
      "         [[False,  True,  True,  True],\n",
      "          [False, False,  True,  True],\n",
      "          [False, False, False,  True],\n",
      "          [False, False, False, False]],\n",
      "\n",
      "         [[False,  True,  True,  True],\n",
      "          [False, False,  True,  True],\n",
      "          [False, False, False,  True],\n",
      "          [False, False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True,  True,  True],\n",
      "          [False, False,  True,  True],\n",
      "          [False, False, False,  True],\n",
      "          [False, False, False, False]],\n",
      "\n",
      "         [[False,  True,  True,  True],\n",
      "          [False, False,  True,  True],\n",
      "          [False, False, False,  True],\n",
      "          [False, False, False, False]],\n",
      "\n",
      "         [[False,  True,  True,  True],\n",
      "          [False, False,  True,  True],\n",
      "          [False, False, False,  True],\n",
      "          [False, False, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "# creating mask\n",
    "mask = torch.triu(torch.ones((seq, seq), dtype=torch.bool), diagonal=1) # (seq, seq)\n",
    "# expanding mask across batch and heads dimensions\n",
    "mask = mask.unsqueeze(0).unsqueeze(0).expand(b, n_heads, -1, -1) # (b,n_heads, seq, seq )\n",
    "print(f\"S:{mask.shape}\\n{mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9644aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:torch.Size([2, 3, 4, 4])\n",
      "tensor([[[ 0.2569, -0.2507,  0.0530,  0.0344],\n",
      "         [ 0.0221, -0.5462, -0.1012,  0.1261],\n",
      "         [-0.2866, -0.0408, -0.2315, -0.1548],\n",
      "         [-0.6317, -0.2327, -0.2567,  0.1943]],\n",
      "\n",
      "        [[ 0.4582,  0.8343,  1.8337, -0.7559],\n",
      "         [ 0.0463,  0.6201,  0.7675, -0.1347],\n",
      "         [ 0.1563, -0.1418, -0.5324,  0.1354],\n",
      "         [ 0.2650,  0.9619, -0.3649,  0.4824]],\n",
      "\n",
      "        [[-0.0670, -0.0365, -0.0631, -0.4189],\n",
      "         [ 0.2537,  0.5420,  0.5959,  1.8983],\n",
      "         [ 0.2429, -0.4177, -0.3470,  0.4181],\n",
      "         [ 0.2617,  0.0530,  0.0512,  0.6890]]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# compute attention score \n",
    "mh_attention_score = Q @ K.transpose(2,3)\n",
    "print(f\"S:{mh_attention_score.shape}\\n{mh_attention_score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "641bba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:torch.Size([2, 3, 4, 4])\n",
      "tensor([[[ 0.2569,    -inf,    -inf,    -inf],\n",
      "         [ 0.0221, -0.5462,    -inf,    -inf],\n",
      "         [-0.2866, -0.0408, -0.2315,    -inf],\n",
      "         [-0.6317, -0.2327, -0.2567,  0.1943]],\n",
      "\n",
      "        [[ 0.4582,    -inf,    -inf,    -inf],\n",
      "         [ 0.0463,  0.6201,    -inf,    -inf],\n",
      "         [ 0.1563, -0.1418, -0.5324,    -inf],\n",
      "         [ 0.2650,  0.9619, -0.3649,  0.4824]],\n",
      "\n",
      "        [[-0.0670,    -inf,    -inf,    -inf],\n",
      "         [ 0.2537,  0.5420,    -inf,    -inf],\n",
      "         [ 0.2429, -0.4177, -0.3470,    -inf],\n",
      "         [ 0.2617,  0.0530,  0.0512,  0.6890]]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# apply mask to prevent looking ahead (causla attention)\n",
    "mh_attention_score.masked_fill_(mask, -torch.inf)\n",
    "print(f\"S:{mh_attention_score.shape}\\n{mh_attention_score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06516d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:torch.Size([2, 3, 4, 4])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5813, 0.4187, 0.0000, 0.0000],\n",
      "         [0.3140, 0.3619, 0.3241, 0.0000],\n",
      "         [0.1956, 0.2463, 0.2429, 0.3152]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4179, 0.5821, 0.0000, 0.0000],\n",
      "         [0.3978, 0.3349, 0.2673, 0.0000],\n",
      "         [0.2313, 0.3458, 0.1607, 0.2622]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4585, 0.5415, 0.0000, 0.0000],\n",
      "         [0.4177, 0.2852, 0.2971, 0.0000],\n",
      "         [0.2468, 0.2188, 0.2186, 0.3159]]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# scale and apply softmax\n",
    "attention_weight = F.softmax(mh_attention_score / (head_dim ** 0.5), dim=-1) # (b, n_heds, seq, seq)\n",
    "print(f\"Shape:{attention_weight.shape}\\n{attention_weight[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "865d9a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Dropout:\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.9688, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5233, 0.6031, 0.0000, 0.0000],\n",
      "         [0.3260, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.6667, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.9701, 0.0000, 0.0000],\n",
      "         [0.6630, 0.5582, 0.4455, 0.0000],\n",
      "         [0.0000, 0.5764, 0.0000, 0.4370]],\n",
      "\n",
      "        [[1.6667, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7641, 0.9025, 0.0000, 0.0000],\n",
      "         [0.0000, 0.4754, 0.0000, 0.0000],\n",
      "         [0.4113, 0.0000, 0.0000, 0.0000]]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# lets apply dropout \n",
    "attention_weight = dropout(attention_weight)\n",
    "print(f'After Dropout:\\n{attention_weight[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1699ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:torch.Size([2, 3, 4, 3])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5083,  0.6182,  0.3072],\n",
      "         [ 0.4235,  0.4775, -0.2888],\n",
      "         [ 0.1710,  0.2080,  0.1034]],\n",
      "\n",
      "        [[ 1.4620,  0.1345, -1.6095],\n",
      "         [ 0.1398, -0.9007, -0.8149],\n",
      "         [ 0.9255, -0.4715, -1.2611],\n",
      "         [-0.2885, -0.5021,  0.0798]],\n",
      "\n",
      "        [[ 0.3246,  0.7681,  0.8544],\n",
      "         [ 1.0397,  1.1539,  0.8462],\n",
      "         [ 0.4692,  0.4223,  0.2394],\n",
      "         [ 0.0801,  0.1896,  0.2109]]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# compute attention output V @ attention weight\n",
    "context_vector = attention_weight @ V # (b, n_heads, seq, head_dim)\n",
    "print(f\"Shape:{context_vector.shape}\\n{context_vector[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c126566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:torch.Size([2, 4, 9])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.5083,  0.6182,  0.3072,  0.4235,  0.4775,\n",
      "         -0.2888],\n",
      "        [ 0.1710,  0.2080,  0.1034,  1.4620,  0.1345, -1.6095,  0.1398, -0.9007,\n",
      "         -0.8149],\n",
      "        [ 0.9255, -0.4715, -1.2611, -0.2885, -0.5021,  0.0798,  0.3246,  0.7681,\n",
      "          0.8544],\n",
      "        [ 1.0397,  1.1539,  0.8462,  0.4692,  0.4223,  0.2394,  0.0801,  0.1896,\n",
      "          0.2109]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now lets combine the heads d_model = n_heads * head_dim. Mostly the tensor is contiguous after all these operations, but just to be safe we can use .contiguous()\n",
    "context_vector = context_vector.contiguous().view(b, seq, d_model) # (b, seq, d_model)\n",
    "print(f\"Shape:{context_vector.shape}\\n{context_vector[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f1b52",
   "metadata": {},
   "source": [
    "**Multi Head attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ae575f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, qkv_bias=True, dropout=0.3):\n",
    "        # here we could have used d_in, and d_out. But the assumption is d_models is the same as the input size embedding and desired output\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = int(d_model / n_heads)\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq, d_model = x.size()\n",
    "        assert self.d_model == d_model, \"Input size must be equal as passed into the model\"\n",
    "        Q = self.wq(x) # (b,seq,d_model) \n",
    "        K = self.wk(x) # (b,seq,d_model)\n",
    "        V = self.wv(x) # (b,seq,d_model)\n",
    "\n",
    "        # split heads and transpose \n",
    "        # (b,seq,d_model) -> (b, seq, n_heads, head_dim) -> (b, n_heads, seq, head_dim)\n",
    "        Q = Q.view(b, seq, self.n_heads, self.head_dim).transpose(1, 2) \n",
    "        K = K.view(b, seq, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        V = V.view(b, seq, self.n_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # compute the attention scores\n",
    "        attn_scores = Q @ K.transpose(2,3)\n",
    "        # create a boolean mask of (seq, seq)\n",
    "        mask = torch.triu(torch.ones((seq, seq), dtype=torch.bool), diagonal=1)\n",
    "        # expand it to match the attention scores shape (b, n_heads, seq, seq)\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0).expand(b, self.n_heads, -1, -1)\n",
    "\n",
    "        # create masked attention score \n",
    "        attn_scores = attn_scores.masked_fill_(mask, -torch.inf)\n",
    "\n",
    "        # compute the scales attention weights \n",
    "        attn_weights = F.softmax(attn_scores / (self.head_dim ** 0.5), dim=-1)\n",
    "\n",
    "        # apply dropout to the attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # compute the output of the scaled dot product attention\n",
    "        # (b, n_heads, seq, seq) @ (b, n_heads, seq, head_dim) -> (b, n_heads, seq, head_dim)\n",
    "        context_vec = attn_weights @ V\n",
    "        # (b, seq, n_heads, head_dim)\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "        #reshape it back to (b, seq, d_model)\n",
    "        context_vec = context_vec.contiguous().view(b, seq, self.d_model)\n",
    "        output = self.wo(context_vec)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eeaf8d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:torch.Size([2, 4, 9])\n",
      "tensor([[ 0.3305,  0.8490, -0.7650, -0.5499, -0.5673,  0.6997,  0.4305, -1.4169,\n",
      "         -0.1181],\n",
      "        [ 0.0656,  0.1773, -0.2291, -0.2755,  0.0037,  0.8044, -0.2406, -0.8244,\n",
      "         -0.1588],\n",
      "        [-0.2307,  0.1386,  0.0015, -0.3392,  0.2659,  0.7047, -0.2284, -0.4015,\n",
      "         -0.2836],\n",
      "        [ 0.3415,  0.0311, -0.0630, -0.3728,  0.0957,  0.3276,  0.0122, -0.6248,\n",
      "         -0.0620]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(n_heads, d_model)\n",
    "output = mha(input)\n",
    "print(f\"Shape:{output.shape}\\n{output[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395340c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
