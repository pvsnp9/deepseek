{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9162b5b2",
   "metadata": {},
   "source": [
    "#### Next Token Prediction Task (**KV Cache**)\n",
    "\n",
    "![Inference](media/Inference.jpg)\n",
    "\n",
    "In transformer models, self-attention computes attention scores over the entire input sequence for every token generated or processed.\n",
    "\n",
    "![QKV](media/QKV.jpg)\n",
    "\n",
    "- For each new token generated (e.g., in autoregressive decoding), you compute new Query (Q), Key (K), and Value (V) vectors for the entire sequence so far.\n",
    "- This means recomputing K and V for all past tokens repeatedly, which is very inefficient because past tokens don't change during generation.\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "- Computational inefficiency: Computing K and V repeatedly for all previous tokens leads to quadratic time complexity with respect to sequence length during generation.\n",
    "- Latency: Slows down autoregressive generation in tasks like language modeling or translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cdfec",
   "metadata": {},
   "source": [
    "**KV Cache**\n",
    "\n",
    "- Cache means to store and reuse.\n",
    "- Instead of recomputing `K` and `V` for all previous tokens every step, we store (cache) the `K` and `V` vectors computed at each step.\n",
    "- When generating the next token, we only compute the `Q` vector for the new token, and reuse the cached `K` and `V` vectors from all previous tokens.\n",
    "- This drastically reduces computation, since K and V don't have to be recomputed for the entire history.\n",
    "\n",
    "\n",
    "*Illustration:*\n",
    "| Step | Tokens processed          | Compute Q for | Compute K, V for | Use cached K, V for     |\n",
    "| ---- | ------------------------- | ------------- | ---------------- | ----------------------- |\n",
    "| 1    | \\[token1]                 | token1        | token1           | token1                  |\n",
    "| 2    | \\[token1, token2]         | token2        | token2           | token1 (cached)         |\n",
    "| 3    | \\[token1, token2, token3] | token3        | token3           | token1, token2 (cached) |\n",
    "\n",
    "Without cache, at step 3, we recompute K and V for all 3 tokens. With KV cache, at step 3, we reuse K1,K2,V1,V2 from cache, only compute K3,V3.\n",
    "\n",
    "\n",
    "![Kv Cache](media/kvcache.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512fd1b",
   "metadata": {},
   "source": [
    "**Simple Multi Head Attention with KV Cache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68bee0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7c83fe7ac0d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07dff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMultiHeadAttentionWithKV(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, context_len, qkv_bias=False, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num heads\" \n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = int(d_model / num_heads)\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "\n",
    "        # This is not required for inference, Yet will be auto disabled during inference. \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # cache KV\n",
    "        self.cached_key = None\n",
    "        self.cached_value = None\n",
    "\n",
    "        # mask \n",
    "        #This not efficient, better to do it on demand. Move to forward section\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_len, context_len), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq, d_model = x.size()\n",
    "\n",
    "        Q = self.wq(x) # (b, seq, d_model)\n",
    "        K = self.wk(x) # (b, seq, d_model)\n",
    "        V = self.wv(x) # (b, seq, d_model)\n",
    "\n",
    "        # split heads (b, seq, d_mdoel) -> (b, num_heads, seq, head_dim)\n",
    "        Q = Q.view(b, seq, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = K.view(b, seq, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = V.view(b, seq, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # check if have cached KV\n",
    "        if self.cached_key is None or self.cached_value is None:\n",
    "            self.cached_key = K\n",
    "            self.cached_value = V\n",
    "        else:\n",
    "            # update cache original cached (b, num_heads, seq, head_dim) concatented with seq dim = 2\n",
    "            self.cached_key = torch.cat((self.cached_key, K), dim = 2)\n",
    "            self.cached_value = torch.cat((self.cached_value, V), dim = 2)\n",
    "        \n",
    "        # compute score (b, num_heads, seq, head_dim) @ (b, num_heads, head_dim, seq) -> (b, num_heads, seq, seq)\n",
    "        attn_score = Q @ self.cached_key.transpose(2,3)\n",
    "        mask = self.mask.unsqueeze(0).unsqueeze(0).expand(b, self.num_heads, -1, -1)\n",
    "        # pluckking out the seq elements only form context length\n",
    "        mask = mask[:,:,:seq,:seq]\n",
    "        attn_score.masked_fill_(mask, -torch.inf)\n",
    "\n",
    "        # compute scaled attention weight \n",
    "        attn_weight = F.softmax(attn_score / (self.head_dim **0.5), dim=-1)\n",
    "        # dropuout \n",
    "        attn_weight = self.dropout(attn_weight)\n",
    "        \n",
    "        context_vector = attn_weight @ self.cached_value\n",
    "        # reshape (b, num_heads, seq, seq) -> (b, seq, d_model)\n",
    "        context_vector = context_vector.transpose(1,2).view(b, seq, self.d_model)\n",
    "\n",
    "        output = self.wo(context_vector)\n",
    "        return output, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdca8e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass: 1, K:torch.Size([2, 2, 1, 2]) V:torch.Size([2, 2, 1, 2])\n",
      "Out:torch.Size([2, 1, 4]), attn_eight:torch.Size([2, 2, 1, 1])\n",
      "------------------------------------------------------------\n",
      "Pass: 2, K:torch.Size([2, 2, 2, 2]) V:torch.Size([2, 2, 2, 2])\n",
      "Out:torch.Size([2, 1, 4]), attn_eight:torch.Size([2, 2, 1, 2])\n",
      "------------------------------------------------------------\n",
      "Pass: 3, K:torch.Size([2, 2, 3, 2]) V:torch.Size([2, 2, 3, 2])\n",
      "Out:torch.Size([2, 1, 4]), attn_eight:torch.Size([2, 2, 1, 3])\n",
      "------------------------------------------------------------\n",
      "Pass: 4, K:torch.Size([2, 2, 4, 2]) V:torch.Size([2, 2, 4, 2])\n",
      "Out:torch.Size([2, 1, 4]), attn_eight:torch.Size([2, 2, 1, 4])\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "token_one = torch.randn(2, 1, 4)\n",
    "token_two = torch.randn(2, 1, 4)\n",
    "token_three = torch.randn(2, 1, 4)\n",
    "\n",
    "b, seq, d_model = [2, 1, 4]\n",
    "num_heads = 2\n",
    "context_len = 4\n",
    "num_pass = 4\n",
    "\n",
    "kv_model = SimpleMultiHeadAttentionWithKV(d_model, num_heads, context_len )\n",
    "\n",
    "for i in range(num_pass):\n",
    "    data = torch.randn(b, seq, d_model)\n",
    "    out, attn_weight = kv_model(data)\n",
    "    print(f\"Pass: {i+1}, K:{kv_model.cached_key.shape} V:{kv_model.cached_value.shape}\")\n",
    "    print(f\"Out:{out.shape}, attn_eight:{attn_weight.shape}\")\n",
    "    print(\"--\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6823495",
   "metadata": {},
   "source": [
    "This demonstrates the working of a multi-head attention module with a KV cache mechanism implemented in PyTorch. The key highlights of this example include:\n",
    "\n",
    "* The model processes inputs of shape `[batch_size=2, seq_len=1, d_model=4]` with `num_heads=2` and a context length of 4.\n",
    "* In a loop running for 4 passes, new random input data simulating tokens is fed sequentially into the model.\n",
    "* The KV cache (`cached_key` and `cached_value`) accumulates over the passes, expanding the sequence dimension.\n",
    "* After each pass, the shapes of the cached keys and values, as well as the output and attention weights, are printed.\n",
    "* The cache shape grows along the sequence dimension with each new token processed, illustrating how the cache accumulates previous tokens' K and V projections.\n",
    "* The output shape remains `[2, 1, 4]` indicating a single output per token processed.\n",
    "* Attention weights shape grows from `[2, 2, 1, 1]` to `[2, 2, 1, 4]`, showing the expanding attention window over the cached keys.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "This example effectively illustrates how KV caching enables efficient incremental processing in multi-head attention. By caching keys and values across passes, the model avoids recomputing these projections for previous tokens, improving computational efficiency during autoregressive generation or streaming scenarios.\n",
    "\n",
    "* The progressive increase in cached K and V shapes corresponds to the growing context as more tokens are processed.\n",
    "* The stable output shape per token reflects correct incremental output production.\n",
    "* This approach is crucial for scaling transformer-based models to long sequences without quadratic cost in computation.\n",
    "* Managing and verifying the KV cache shape is important to ensure proper attention computation over the accumulated context.\n",
    "\n",
    "Overall, KV caching is a key optimization for transformer inference, enabling faster generation while maintaining correctness, as clearly demonstrated in this practical code example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126abbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
