{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108352dd",
   "metadata": {},
   "source": [
    "**Lets be Normal**\n",
    "#### Normalization \n",
    "\n",
    "![Normalization](media/norm.jpg)\n",
    "\n",
    "\n",
    "**Understanding Layer Normalization Through a Soccer Story**\n",
    "\n",
    "Imagine a soccer team composed of three groups of players: **Defenders**, **Midfielders**, and a **Striker**. Each group represents a layer in a neural network, and the ultimate goal of the team is to score a **goal** — but crucially, the goal isn’t just whether they score; it’s **how well-timed** the goal is.\n",
    "\n",
    "**The Game Setup: Timing is Everything**\n",
    "\n",
    "The team tries to learn how to score a goal **at the right moment**. The coach watches the timing carefully, measuring how many seconds the team misses the perfect timing by. For instance, if they score too late or too early, it counts as missing by some seconds.\n",
    "\n",
    "---\n",
    "\n",
    "*First Pass: The Timing Miss*\n",
    "\n",
    "On the first attempt, the team scores a goal but misses the perfect timing by **2 seconds**. The coach gives feedback:\n",
    "\n",
    "> “You need to play faster to get the timing right.”\n",
    "\n",
    "All players — defenders, midfielders, and striker — agree to play faster.\n",
    "\n",
    "---\n",
    "\n",
    "*Second Pass: Overcompensation and Oscillation*\n",
    "\n",
    "Trying to play faster, the **defenders** kick the ball too far ahead to the midfielders, making it harder for the midfielders to control the ball properly. The coordination between groups breaks down, and the team **fails to score properly**.\n",
    "\n",
    "The team is oscillating — defenders try one extreme, midfielders react differently, and the striker is out of sync. The feedback loop causes instability and makes it **take a long time for the team to learn proper coordination**.\n",
    "\n",
    "---\n",
    "\n",
    "**What’s Happening Here in Neural Network Terms?**\n",
    "\n",
    "* The players are like **neurons in different layers**.\n",
    "* The timing feedback is like the **loss signal** measured after the output.\n",
    "* The team’s attempt to “play faster” without considering the whole coordination is similar to how **changing activations without normalization** can cause instability.\n",
    "* The oscillation and slow learning represent the challenge of **internal covariate shift** — layers have to constantly adapt to changing inputs from previous layers.\n",
    "\n",
    "---\n",
    "\n",
    "**How Layer Normalization Helps**\n",
    "\n",
    "**Layer Normalization** acts like a **coach who normalizes each player’s performance individually before passing the ball**:\n",
    "\n",
    "* It ensures each player (neuron) performs at a **standardized level** regardless of how others perform.\n",
    "* This **stabilizes the inputs** going into each layer, so players are always \"on the same page\".\n",
    "* The team can adjust their pace and coordination **more smoothly and reliably**.\n",
    "* As a result, the team learns faster and scores goals at the right time with fewer oscillations.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Without LayerNorm, the team struggles with timing and coordination, leading to unstable and slow learning. With LayerNorm, each player adjusts their game consistently, allowing the team to synchronize, learn faster, and hit their target timing accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371ebac1",
   "metadata": {},
   "source": [
    "#### Why we need td normalization in deep learning?\n",
    "\n",
    "1. **Problem: Internal Covariate Shift**\n",
    "\n",
    "- During training, the distribution of activations in intermediate layers keeps changing as model weights update.\n",
    "- This forces each layer to constantly adapt to changing input distributions — called internal covariate shift.\n",
    "- This slows down training and makes it harder for the model to converge.\n",
    "\n",
    "2. **Vanishing / Exploding Gradients**\n",
    "\n",
    "- Without normalization, activation values can grow too large or shrink too small.\n",
    "- This leads to vanishing gradients (too small to learn) or exploding gradients (unstable updates).\n",
    "- Normalization helps keep activations in a stable range, improving gradient flow.\n",
    "\n",
    "3. **Faster and More Stable Training**\n",
    "\n",
    "- Normalization techniques (BatchNorm, LayerNorm, RMSNorm, etc.) standardize activations, reducing variation.\n",
    "- This helps networks train faster, be more stable, and often generalize better.\n",
    "\n",
    "\n",
    "\n",
    "**Common Normalization Techniques**\n",
    "| Technique | Normalization Dimension            | Key Idea                                          | Usage                                      |\n",
    "| --------- | ---------------------------------- | ------------------------------------------------- | ------------------------------------------ |\n",
    "| BatchNorm | Normalize over batch dimension     | Normalize mean/std over batch                     | CNNs, image models, during training mostly |\n",
    "| LayerNorm | Normalize per sample, all features | Normalize over features per sample                | Transformers, RNNs, robust to batch size   |\n",
    "| RMSNorm   | Normalize using RMS of features    | Normalize by root mean square (no mean centering) | Efficient, popular in transformers         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92054c1",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9448b9e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### RMSNorm — What is It and Why?\n",
    "\n",
    "RMSNorm normalizes the input vector by its root mean square (RMS) value instead of mean and variance:\n",
    "\n",
    "$$\n",
    "\\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2}\n",
    "$$\n",
    "\n",
    "Normalized output:\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\frac{x}{\\text{RMS}(x)} \\odot g\n",
    "$$\n",
    "\n",
    "where $g \\in \\mathbb{R}^d$ is a learned scaling parameter (element-wise).\n",
    "\n",
    "---\n",
    "\n",
    "**Why RMSNorm?**\n",
    "\n",
    "* **No mean subtraction** unlike LayerNorm — simpler, fewer computations.\n",
    "* Normalizes magnitude, keeping the input vector length stable.\n",
    "* Works well in transformer architectures, sometimes more stable than LayerNorm.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Suppose input vector**:\n",
    "\n",
    "$$\n",
    "x = [3, 4]\n",
    "$$\n",
    "\n",
    "Dimension $d = 2$.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1: Compute RMS**\n",
    "\n",
    "$$\n",
    "\\text{RMS}(x) = \\sqrt{\\frac{3^2 + 4^2}{2}} = \\sqrt{\\frac{9 + 16}{2}} = \\sqrt{\\frac{25}{2}} = \\sqrt{12.5} \\approx 3.535\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Normalize vector**\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\frac{x}{\\text{RMS}(x)} = \\left[\\frac{3}{3.535}, \\frac{4}{3.535}\\right] \\approx [0.849, 1.131]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3: Scale with learned parameter $g = [g_1, g_2]$**\n",
    "\n",
    "Assume $g = [1, 1]$ (no scaling initially):\n",
    "\n",
    "$$\n",
    "y = \\hat{x} \\odot g = [0.849, 1.131]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "| Aspect         | Normalization Purpose              | RMSNorm Specifics                             |\n",
    "| -------------- | ---------------------------------- | --------------------------------------------- |\n",
    "| Problem solved | Stabilizes activations & gradients | Normalizes magnitude without mean subtraction |\n",
    "| Computation    | Mean & variance or RMS             | Only RMS, simpler, efficient                  |\n",
    "| Use case       | Deep learning, transformers        | Effective alternative to LayerNorm            |\n",
    "| Benefits       | Faster, stable training            | Reduced computation, often better stability   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc59b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x75605ffa5fb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c28370",
   "metadata": {},
   "source": [
    "- Lets compute RMS per sample (last dimension).\n",
    "- Normalize by dividing input by RMS.\n",
    "- Scale by learnable parameter $g$\n",
    "- Small epsilon added for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca0a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim:int, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x:torch.Tensor)-> torch.Tensor:\n",
    "        rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) +self.eps)\n",
    "        x_norm = x / rms\n",
    "        return self.g * x_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a3d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:torch.Size([2, 2, 3])\n",
      "tensor([[[-0.2992, -1.1023,  1.3021],\n",
      "         [-1.2794,  0.2985, -1.1287]],\n",
      "\n",
      "        [[-1.0082,  1.3676, -0.3365],\n",
      "         [ 1.3013,  0.5074,  1.0242]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dim = 3\n",
    "inputs = torch.randn(2, 2, dim)\n",
    "rms_norm = RMSNorm(dim)\n",
    "outputs = rms_norm(inputs)\n",
    "print(f\"Shape:{outputs.shape}\\n{outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6350c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
